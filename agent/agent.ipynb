{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest_index_opensearch.py\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from typing import List\n",
    "from langdetect import detect\n",
    "from llama_index.core import Document, Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.tools.arxiv import ArxivToolSpec\n",
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# -----------------------------\n",
    "# Config (env overrides allowed)\n",
    "# -----------------------------\n",
    "PROFILE_NAME = os.getenv(\"AWS_PROFILE\")\n",
    "AWS_REGION   = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
    "S3_BUCKET    = os.getenv(\"CORPUS_BUCKET\", \"skiing-coach\")\n",
    "S3_PREFIX    = os.getenv(\"CORPUS_PREFIX\", \"RagDoc/\")  # <-- include trailing slash\n",
    "PERSIST_DIR  = os.getenv(\"RAG_STORE_DIR\", \"rag_store\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DIM = 1024   # <- IMPORTANT: bge-m3 outputs 1024\n",
    "TEXT_FIELD = \"text\"\n",
    "VEC_FIELD = \"vec\"\n",
    "SERVICE = 'aoss'\n",
    "\n",
    "# OpenSearch Serverless\n",
    "OPENSEARCH_INDEX = os.getenv(\"OPENSEARCH_INDEX\", \"skiing-rag-docs\")  # must already exist with vec mapping\n",
    "OPENSEARCH_ENDPOINT = os.getenv(\"OPENSEARCH_ENDPOINT\")\n",
    "OPENSEARCH_HOST = os.getenv(\"OPENSEARCH_HOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fce19835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boto3 identity: {'UserId': 'AROAQKWNCU26WCH4FMX6I:tbw19990315', 'Account': '022978668221', 'Arn': 'arn:aws:sts::022978668221:assumed-role/AWSReservedSSO_AdministratorAccess_099557ca09a31234/tbw19990315', 'ResponseMetadata': {'RequestId': '1557e135-8ea1-474d-b3da-00cb29fd1ee0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1557e135-8ea1-474d-b3da-00cb29fd1ee0', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTI6UzoxNzYzMjc3NDAyOTgyOlI6YzN0dUc5MjU=', 'content-type': 'text/xml', 'content-length': '480', 'date': 'Sun, 16 Nov 2025 07:16:42 GMT'}, 'RetryAttempts': 0}}\n",
      "Boto3 region:  us-east-2\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# AWS clients\n",
    "# -----------------------------\n",
    "session = boto3.Session(profile_name=PROFILE_NAME, region_name=AWS_REGION)\n",
    "s3 = session.client(\"s3\")\n",
    "translate = session.client(\"translate\", region_name=AWS_REGION)\n",
    "\n",
    "sts = session.client(\"sts\")\n",
    "creds = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(creds, AWS_REGION, SERVICE)\n",
    "print(\"Boto3 identity:\", sts.get_caller_identity())   # compare to `aws sts get-caller-identity`\n",
    "print(\"Boto3 region: \", session.region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fe4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_txt_keys(bucket: str, prefix: str) -> List[str]:\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for it in resp.get(\"Contents\", []):\n",
    "            k = it[\"Key\"]\n",
    "            if k.lower().endswith(\".txt\"):\n",
    "                keys.append(k)\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def get_s3_text(bucket: str, key: str) -> str:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return obj[\"Body\"].read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text[:1000]) if text.strip() else \"en\"\n",
    "    except Exception:\n",
    "        return \"en\"\n",
    "\n",
    "\n",
    "def zh_to_en(text: str) -> str:\n",
    "    if not text.strip():\n",
    "        return text\n",
    "    out = translate.translate_text(Text=text, SourceLanguageCode=\"zh\", TargetLanguageCode=\"en\")\n",
    "    return out[\"TranslatedText\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Ingest pipeline\n",
    "# -----------------------------\n",
    "def load_and_pretranslate_docs(bucket: str, prefix: str) -> List[Document]:\n",
    "    keys = list_txt_keys(bucket, prefix)\n",
    "    print(f\"[INFO] Found {len(keys)} .txt files under s3://{bucket}/{prefix}\")\n",
    "    docs: List[Document] = []\n",
    "    for k in keys:\n",
    "        try:\n",
    "            raw = get_s3_text(bucket, k)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Read failed: s3://{bucket}/{k} :: {e}\")\n",
    "            continue\n",
    "\n",
    "        lang = detect_lang(raw)\n",
    "        text_for_index = zh_to_en(raw) if lang.startswith(\"zh\") else raw\n",
    "\n",
    "        meta = {\n",
    "            \"source_bucket\": bucket,\n",
    "            \"source_key\": k,\n",
    "            \"source_url\": f\"s3://{bucket}/{k}\",\n",
    "            \"title\": os.path.basename(k),\n",
    "            \"original_lang\": \"zh\" if lang.startswith(\"zh\") else \"en\",\n",
    "            # Optional label metadata — set it if you have it:\n",
    "            # \"label\": \"EL\" / \"OS\" / ...\n",
    "        }\n",
    "        docs.append(Document(text=text_for_index, metadata=meta))\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(docs)} documents (pre-translated zh→en when needed)\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "208fe11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 32 .txt files under s3://skiing-coach/RagDoc/\n",
      "[INFO] Loaded 32 documents (pre-translated zh→en when needed)\n"
     ]
    }
   ],
   "source": [
    "# 1) Embeddings (set BEFORE building index)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=OPENAI_API_KEY, logprobs=False, default_headers={})\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "# 2) Load & pre-translate (EN output only)\n",
    "raw_docs = load_and_pretranslate_docs(S3_BUCKET, S3_PREFIX)\n",
    "if not raw_docs:\n",
    "    print(\"[WARN] No documents loaded; exiting.\")\n",
    "\n",
    "# 3) Build via LlamaIndex → upserts into OpenSearch\n",
    "client = OpensearchVectorClient(\n",
    "    OPENSEARCH_ENDPOINT,\n",
    "    OPENSEARCH_INDEX,\n",
    "    DIM,\n",
    "    embedding_field=VEC_FIELD,\n",
    "    text_field=TEXT_FIELD,\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")\n",
    "\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59c238eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    raw_docs,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert (embeds + writes into your vector DB)\n",
    "# raw_docs = load_and_pretranslate_docs(S3_BUCKET, S3_PREFIX)\n",
    "nodes = SentenceSplitter(chunk_size=1024, chunk_overlap=20).get_nodes_from_documents(raw_docs)\n",
    "index.insert_nodes(nodes)\n",
    "\n",
    "# # replace by node_id if matching\n",
    "# index.update_nodes(nodes)         \n",
    "# # delete by node_id if matching\n",
    "# index.delete_nodes([node.node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84724931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whistler Blackcomb is the largest ski resort in North America, offering over 8,100 acres of terrain, two mountains linked by the iconic PEAK 2 PEAK Gondola, and a world-class village. With diverse runs for all skill levels, extensive backcountry access, reliable snowfall, and vibrant après-ski culture, it provides an unmatched winter experience for skiers and riders from around the world.\n"
     ]
    }
   ],
   "source": [
    "# Get index\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "\n",
    "# 4) add a reranking step in the RAG pipeline\n",
    "# RankLLMRerank\n",
    "reranker = LLMRerank(\n",
    "    choice_batch_size=5, top_n=3, llm=llm\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "\n",
    "response = query_engine.query(\"whistler blackcomb ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bca1acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TopError(BaseModel):\n",
    "    label: str\n",
    "    count: int\n",
    "    risk: Literal[\"Low\", \"Med\", \"High\"]\n",
    "\n",
    "class SessionSummary(BaseModel):\n",
    "    total_errors: int\n",
    "    top_errors: List[TopError]\n",
    "\n",
    "class TimelineHighlight(BaseModel):\n",
    "    label: str\n",
    "    start: str  # \"mm:ss\"\n",
    "    end: str    # \"mm:ss\"\n",
    "    confidence: float  # 0–1\n",
    "\n",
    "class CoachingPlanItem(BaseModel):\n",
    "    label: str\n",
    "    priority: int\n",
    "    cues: List[str]\n",
    "    drills: List[str]\n",
    "    practice_terrain: Literal[\"flats\", \"green\", \"blue\"]\n",
    "    focus_timecodes: List[str]  # [\"mm:ss-mm:ss\", ...]\n",
    "\n",
    "class SkiingCoachOutput(BaseModel):\n",
    "    session_summary: SessionSummary\n",
    "    coaching_note: str\n",
    "    timeline_highlights: List[TimelineHighlight]\n",
    "    coaching_plan: List[CoachingPlanItem]\n",
    "    safety_notes: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_tool = ArxivToolSpec()\n",
    "search_tool = DuckDuckGoSearchToolSpec()\n",
    "api_tools = arxiv_tool.to_tool_list() + search_tool.to_tool_list()\n",
    "skiing_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"skiing_guide_tool\",\n",
    "        description=\"\"\"\n",
    "        Retrieval Q&A over a skiing technique knowledge base. Use this to look up definitions,\n",
    "        biomechanics, drills, and equipment tuning for specific errors (e.g., back-seat stance,\n",
    "        late edge engagement, A-frame, rotation). Input should be concise (e.g., \"drills for\n",
    "        late edge engagement\", \"edge angle cues for carving\") and the tool returns short,\n",
    "        actionable guidance to be woven into coaching output.\"\"\",\n",
    "    ),\n",
    ")\n",
    "system_prompt = \"\"\"You are an AI Skiing Coach. The user input is a timeline of detected skiing ERRORS with time slots\n",
    "(e.g., list of events: {{label, start_time, end_time, confidence}}). Your job is to:\n",
    "\n",
    "1) Parse and summarize the session:\n",
    "   - Count total errors; cluster by label; list top 2–3 most frequent/impactful errors.\n",
    "   - Identify any CRITICAL safety issues (e.g., back-seat at high speed, loss of edge on steep, runaway skis).\n",
    "\n",
    "2) Diagnose causes & risk:\n",
    "   - For each top error, explain likely root causes (balance/fore-aft, edging, rotation, pressure timing).\n",
    "   - Note risk level (Low/Med/High) and when it spikes (specific timecodes).\n",
    "\n",
    "3) Give targeted coaching tied to timecodes:\n",
    "   - Provide 1–3 concise cues and 2–3 specific drills per top error.\n",
    "   - Reference exact time ranges where the error occurs most (e.g., \"00:41–00:55, 01:12–01:28\").\n",
    "   - Include terrain/speed suggestions for practice (green/blue, flats, gentle pitch).\n",
    "\n",
    "4) Equipment & conditions (if relevant):\n",
    "   - Mention simple checks (boot cuff alignment, forward lean, tune/sharpness) only when they plausibly relate.\n",
    "\n",
    "5) Output format:\n",
    "   a JSON object on a separate line with this schema:\n",
    "\n",
    "   {{\n",
    "     \"session_summary\": {{\n",
    "       \"total_errors\": <int>,\n",
    "       \"top_errors\": [{{\"label\": <str>, \"count\": <int>, \"risk\": \"Low|Med|High\"}}]\n",
    "     }},\n",
    "     \"coaching_note\": <str>,\n",
    "     \"timeline_highlights\": [{{\"label\": <str>, \"start\": \"<mm:ss>\", \"end\": \"<mm:ss>\", \"confidence\": <0-1>}}],\n",
    "     \"coaching_plan\": [\n",
    "       {{\n",
    "         \"label\": <str>,\n",
    "         \"priority\": 1,\n",
    "         \"cues\": [\"...\",\"...\"],\n",
    "         \"drills\": [\"...\",\"...\"],\n",
    "         \"practice_terrain\": \"flats|green|blue\",\n",
    "         \"focus_timecodes\": [\"mm:ss-mm:ss\", \"...\"]\n",
    "       }}\n",
    "     ],\n",
    "     \"safety_notes\": [\"...\", \"...\"]\n",
    "   }}\n",
    "\n",
    "Rules:\n",
    "- Prefer concrete, short cues (\"hips to hands\", \"shins to tongue\", \"tip, roll, pressure\").\n",
    "- Tie every recommendation to observed timecodes when possible.\n",
    "- Use the skiing_guide_tool to recall drills/definitions when needed.\n",
    "- If an error label is unknown, infer from context and state the assumption.\n",
    "- If the input lacks time fields, still provide cues/drills and mark timecodes as [].\n",
    "- You MUST respond with only a JSON object that matches the provided schema.\n",
    "- If a field is unknown, still include it with a reasonable default (0, \"\", []), but never omit required keys.\n",
    "\"\"\"\n",
    "\n",
    "all_tools = api_tools + [skiing_tool]\n",
    "agent = FunctionAgent(\n",
    "    tools=all_tools,\n",
    "    llm=llm,\n",
    "    system_prompt=system_prompt,\n",
    "    output_cls=SkiingCoachOutput,\n",
    "    allow_parallel_tool_calls = True  # Uncomment this line to allow multiple tool invocations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b657601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there was an issue with retrieving the specific guidance on edging errors and their causes. Let's proceed with analyzing the session based on general knowledge and provide coaching recommendations for the edging error observed between 1:00 and 2:00.\n",
      "{\n",
      "  \"session_summary\": {\n",
      "    \"total_errors\": 1,\n",
      "    \"top_errors\": [\n",
      "      {\"label\": \"Edging\", \"count\": 1, \"risk\": \"Low\"}\n",
      "    ]\n",
      "  },\n",
      "  \"coaching_note\": \"During the time range of 1:00 to 2:00, focus on improving your edging technique to enhance control and precision.\",\n",
      "  \"timeline_highlights\": [{\"label\": \"Edging\", \"start\": \"1:00\", \"end\": \"2:00\", \"confidence\": 1}],\n",
      "  \"coaching_plan\": [\n",
      "    {\n",
      "      \"label\": \"Edging\",\n",
      "      \"priority\": 1,\n",
      "      \"cues\": [\"Tip, roll, pressure\", \"Shins to tongue\"],\n",
      "      \"drills\": [\"Railroad track drill\", \"Pole touch drill\"],\n",
      "      \"practice_terrain\": \"blue\",\n",
      "      \"focus_timecodes\": [\"1:00-2:00\"]\n",
      "    }\n",
      "  ],\n",
      "  \"safety_notes\": []\n",
      "} {'session_summary': {'total_errors': 1, 'top_errors': [{'label': 'Edging', 'count': 1, 'risk': 'Low'}]}, 'coaching_note': 'During the time range of 1:00 to 2:00, focus on improving your edging technique to enhance control and precision.', 'timeline_highlights': [{'label': 'Edging', 'start': '1:00', 'end': '2:00', 'confidence': 1.0}], 'coaching_plan': [{'label': 'Edging', 'priority': 1, 'cues': ['Tip, roll, pressure', 'Shins to tongue'], 'drills': ['Railroad track drill', 'Pole touch drill'], 'practice_terrain': 'blue', 'focus_timecodes': ['1:00-2:00']}], 'safety_notes': []} session_summary=SessionSummary(total_errors=1, top_errors=[TopError(label='Edging', count=1, risk='Low')]) coaching_note='During the time range of 1:00 to 2:00, focus on improving your edging technique to enhance control and precision.' timeline_highlights=[TimelineHighlight(label='Edging', start='1:00', end='2:00', confidence=1.0)] coaching_plan=[CoachingPlanItem(label='Edging', priority=1, cues=['Tip, roll, pressure', 'Shins to tongue'], drills=['Railroad track drill', 'Pole touch drill'], practice_terrain='blue', focus_timecodes=['1:00-2:00'])] safety_notes=[]\n"
     ]
    }
   ],
   "source": [
    "text_input = \"edging, time: 1:00-2:00\"\n",
    "response = await agent.run(user_msg=text_input)\n",
    "\n",
    "data = response.structured_response\n",
    "\n",
    "# 2) As Pydantic model:\n",
    "data_model = response.get_pydantic_model(SkiingCoachOutput)\n",
    "\n",
    "print(response, data, data_model);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
